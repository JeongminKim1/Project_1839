# Project_1839

1. Crawling (크롤링)
* 아나콘다(https://www.anaconda.com/products/individual )’를 설치하여 파이썬과 라이브러리를 한번에 사용 할 수 있는 가상환경을 만들고 
주피터 노트북(https://jupyter.org/ )으로 실행
* Pandas 라이브러리를 통해 필요한 데이터만을 담은 data frame을 선언하고 이를 통해 머신러닝에 적용시킬 수 있음. 인덱스가 있어 행과 열로 구성된 프레임에 데이터가 저장되는 테이블 형태의 data frame을 이용할 예정
* BeautifulSoup은 html 코드를 파이썬이 인식 할 수 있는 객체 구조로 변환하는 Parsing을 담당
* selenium은 브라우저 자동화도구로서 웹브라우저를 통해 동적페이지에서도 데이터 수집  가능

과정
![1](https://user-images.githubusercontent.com/76679270/145396988-751b64cb-a10d-4afe-8854-5d2307d3e0b5.jpg)
1) Anaconda Prompt
3) 크롬드라이버 저장 경로 ,자신의 네이버 ID/PW입력 
4) 데이터를 저장하려는 엑셀파일의 절대 경로를 입력
5) 스크래핑 되는 과정
6) 스크래핑 완료 후 엑셀파일에 저장된 데이터 모습
